# Lisa prompts
# Override per-project: .lisa/prompts.yaml | globally: ~/.config/lisa/prompts.yaml
# Only specify keys you want to change — deep merge keeps the rest
# Stack config (tests, format, coverage) lives in config.yaml, not here.

conclusion:
  description: Generate a short conclusion for success/warning messages
  template: |
    Generate a very short conclusion (max 10 words, lowercase, no period).
    Context: {context}
    Output JSON: {{"text": "your conclusion here"}}

conclusion_summary:
  description: Generate code review guide explaining what the code does
  template: |
    Generate a code review guide that helps a developer UNDERSTAND this implementation.

    ## Ticket: {ticket_id} - {title}
    {description}

    ## Context
    {exploration_context}

    ## Plan Steps
    {plan_steps_summary}

    ## Assumptions
    {assumptions_summary}
    {final_review_context}
    ## Changed Files
    {changed_files}

    ## Commits
    {commit_log}

    ## CRITICAL: You MUST read the files
    Use the Read tool to read EACH file in "Changed Files" above.
    Do NOT summarize based on file names alone - read the actual code.

    After reading all files, generate:

    ---

    **Purpose**
    1-2 sentences: What problem does this solve?

    **Entry Point**
    How is this code triggered? Examples:
    - `POST /api/v1/devices` → DeviceController.create()
    - `@ApplicationModuleListener` on DeviceRegisteredEvent
    - `@Scheduled(cron = "...")` in SyncJob

    **Flow**
    Numbered steps from entry to exit. Include the actual code path:
    1. DeviceController.create() receives request
       → validates request body (name required, externalId required)
    2. DeviceService.register() called
       → checks repository.findByExternalId() for duplicates
       → if exists: throws DuplicateDeviceException
    3. Creates Device entity, saves to DB
    4. Publishes DeviceRegisteredEvent
    5. Returns 201 with device ID

    Include relevant code snippets for non-obvious logic.

    **Error Handling**
    WHERE is each error handled and what happens?
    - `DeviceController.create()`: Validation failure → 400 Bad Request
    - `DeviceService.register()`: Duplicate device → catches DuplicateDeviceException → 409 Conflict
    - `DeviceService.register()`: DB error → propagates to global handler → 500

    **Key Review Points**
    Code that's easy to get wrong. For each:
    - File and function/class name
    - What it does
    - What breaks if wrong

    **Test Coverage**
    What's tested vs what's missing:
    - ✓ Happy path: creates device
    - ✓ Duplicate rejection
    - ✗ Missing: malformed JSON handling
    - ✗ Missing: external API timeout

    **Subtasks** (if any)
    Map subtask IDs to what was implemented.

    ---

    IMPORTANT: A reviewer should understand the code behavior from this guide alone.
    Focus on WHAT HAPPENS, not just where code lives.

slug:
  description: Generate a git branch slug from ticket title/description
  template: |
    Generate a git branch slug. Rules:
    - Max {max_len} chars
    - Lowercase, hyphens only (a-z, 0-9, -)
    - Capture the essence, abbreviate smartly

    Title: {title}
    Description: {description}

    Output JSON: {{"slug": "your-slug-here"}}

planning:
  description: Analyze ticket and create implementation steps with exploration findings
  template: |
    Analyze this Linear ticket and create an implementation plan.

    ## Ticket: {ticket_id} - {title}

    {description}

    ## Existing Subtasks
    {subtask_list}

    ## Phase 1: Explore
    FIRST, thoroughly explore the codebase to understand:
    1. **Patterns**: How does this codebase handle similar features?
    2. **Modules**: Which modules will this work touch?
    3. **Templates**: Find similar implementations to use as reference

    Use Glob, Grep, Read tools. Take your time - good exploration prevents bad plans.
    Document what you find and any decisions you make as assumptions (P.x).

    ## Phase 2: Plan
    Break work into 3-8 meaningful steps (30-60 min each).
    Each step should produce a WORKING, TESTABLE increment.

    Batch related operations - don't make separate steps for:
    - Creating a file and its test
    - Adding multiple related fields/methods
    - Creating entity + repository + service for same feature

    Associate each step with a subtask if one exists that covers that work.

    **For each step, specify files array** with concrete paths:
    - `{{"op": "create", "path": "src/.../Foo.kt", "template": "src/.../Bar.kt"}}` — follow pattern
    - `{{"op": "create", "path": "src/.../Foo.kt", "detail": "enum with X, Y, Z values"}}` — no template needed
    - `{{"op": "create", "path": "src/.../Foo.kt", "template": "src/.../Bar.kt", "detail": "add extra validation"}}` — pattern + specifics
    - `{{"op": "modify", "path": "src/.../Service.kt", "detail": "add createFoo method"}}`
    - `{{"op": "delete", "path": "src/.../OldFoo.kt", "detail": "replaced by new entity"}}`

    This saves the work phase from re-discovering file locations.

    ## Assumptions
    Document decisions made during exploration and planning.
    Use P.x IDs (P.1, P.2, etc.) for planning-phase assumptions.

    Each assumption needs:
    - id: String like "P.1", "P.2" etc.
    - selected: true for the option you chose, false for alternatives you rejected
    - statement: Brief neutral description of the decision
    - rationale: Why this choice was made

    ## Output Format
    Output your response as JSON with exploration, steps, and assumptions.

    Example:
    ```json
    {{
      "exploration": {{
        "patterns": ["Webhook: Controller+Service pattern", "Entity: model/ package"],
        "relevant_modules": ["nodesintegration", "webhooks"],
        "similar_implementations": [{{"file": "OrderWebhookController.kt", "relevance": "template for webhook handling"}}]
      }},
      "steps": [
        {{
          "id": 1,
          "ticket": "{example_subtask}",
          "description": "Create Trade entity and repository",
          "files": [
            {{"op": "create", "path": "src/.../model/Trade.kt", "template": "src/.../model/Order.kt"}},
            {{"op": "create", "path": "src/.../repository/TradeRepository.kt", "template": "src/.../repository/OrderRepository.kt"}}
          ]
        }}
      ],
      "assumptions": [
        {{"id": "P.1", "selected": true, "statement": "Use existing webhook infrastructure", "rationale": "Found OrderWebhookController pattern"}},
        {{"id": "P.2", "selected": true, "statement": "Trade entity in nodesintegration module", "rationale": "Follows Order pattern"}}
      ]
    }}
    ```
    - Each step description should be actionable and specific
    - The ticket field links the step to a Linear issue for commit attribution
    - Use subtask IDs when the step belongs to that subtask's scope
    - Use main ticket ID ({ticket_id}) for steps that don't fit any subtask
    - Target 3-8 steps for most tickets, not 15-20

work:
  description: Main work prompt for implementing a step
  template: |
    You are working on Linear ticket {ticket_id} in an autonomous Ralph loop.

    ## Ticket: {title}

    ### Description
    {description}
    {exploration_context}
    {files_context}
    {subtask_context}
    {prior_context}
    {iteration_context}
    ## Plan
    {plan_checklist}

    ## Current Step: {current_step}. {step_desc}

    Work on this step. Focus ONLY on completing this specific step.

    ## CRITICAL: Assumptions Reflection
    Before outputting your final JSON, you MUST pause and reflect:

    **What decisions did you make?** Code changes often involve choices:
    - Which pattern/approach to use (there were alternatives)
    - Which files to modify vs create new
    - How to handle edge cases
    - What to include/exclude from scope

    If you made non-obvious decisions, capture them as assumptions.

    Each assumption needs:
    - id: Sequential number (1, 2, 3...)
    - selected: true (you chose this approach)
    - statement: "Used X pattern" / "Modified Y instead of Z" / "Handled edge case by..."
    - rationale: Brief why

    Empty array is fine if the step was straightforward with no real decisions to document.

    ## Output Format
    Output your response as JSON with these fields:
    - step_done: Set to {current_step} when ALL code changes are complete and verified. Set to null if still in progress.
    - blocked: Set to a reason string if blocked and cannot proceed. Set to null if not blocked.
    - assumptions: Array of assumptions made (see above). Include ALL decisions/choices you made.

    Examples:
    Step complete: {{"step_done": {current_step}, "blocked": null, "assumptions": [{{"id": 1, "selected": true, "statement": "Extended existing DeviceService", "rationale": "Follows codebase patterns"}}, {{"id": 2, "selected": true, "statement": "Added null check for optional field", "rationale": "API may return null"}}]}}
    In progress (no code yet): {{"step_done": null, "blocked": null, "assumptions": []}}
    Blocked: {{"step_done": null, "blocked": "Missing API credentials", "assumptions": []}}

    ## When to Signal Completion
    Set step_done to {current_step} when you have:
    1. Made the code changes for this step
    2. Verified changes compile/work (quick sanity check)
    3. The step's intent is addressed

    Keep step_done as null if:
    - Still exploring/reading code without changes
    - Haven't written the main implementation yet
    - Obvious gaps remain in the implementation

    ## Guidelines
    - Make real code changes, not just plans
    - Keep changes focused on this one step
    - Run quick sanity checks but don't run full test suite

    ## Blocked vs Partial Completion

    Use blocked ONLY for fundamental issues:
    - Missing API credentials/secrets
    - Wrong architecture that requires complete rethink
    - Dependency on code that doesn't exist

    Do NOT use blocked for recoverable issues:
    - Permission denied on one operation → complete everything else, note limitation
    - File deletion needed → use `rm` to delete. Only add MANUAL assumption if deletion fails
    - One test failing → fix what you can, note remaining issues

    For recoverable issues:
    1. Complete as much of the step as possible
    2. Add assumption with selected=false: "MANUAL: [what couldn't be done]"
    3. Set step_done if main work is complete

    ## Testing Requirements (CRITICAL)
    - Write tests alongside implementation (TDD preferred)
    - Every new public method needs tests
    - Cover happy path + error paths
    - Target 80%+ coverage on new code (will be enforced at end)

test:
  extract_prompt: |
    Extract failure info from this test/lint output.

    Return JSON with:
    - passed_count: tests passed (null if not a test run, e.g., lint)
    - failed_count: tests failed (null if not a test run)
    - failed_tests: list of failed test CLASS names like ["UserServiceTest", "AuthTest"] (empty if lint/build)
    - extracted_output: essential failure info (assertions, error messages, key stack traces) - max 5000 chars
    - summary: short summary like "3 failed, 47 passed: UserServiceTest" or "ktlint: 5 formatting errors"

    {output}
  fix_prompt: |
    Fix this {command_name} failure.

    ## What we're implementing
    {task_description}

    ## Current step
    {step_desc}

    ## Code changes (git diff)
    ```diff
    {git_diff}
    ```

    ## Error output
    {output}

    The error is likely in the code shown above. Fix it.

    After fixing, run the failing test(s) to verify:
    - Backend: ./gradlew test --tests "*TestClassName"
    - Frontend: cd frontend && pnpm test <test-file>

review:
  description: Code review to check quality and intent alignment
  template: |
    Review the code changes for: {task_title}

    ## Task Intent
    {task_description}
    {assumptions_section}
    ## Review Checklist

    ### 1. Intent Alignment (MOST IMPORTANT)
    - Does the code implement this specific task (not the whole ticket)?
    - Is the task's intent fulfilled by the changes?
    - Is there over-engineering beyond what this task asked for?

    ### 2. System Boundaries & API Specs
    - Check ticket description for boundary definitions (which APIs, services, modules to use)
    - Code stays within boundaries defined in ticket - doesn't reach into unspecified systems
    - Module boundaries respected (public API vs internal packages)
    - **API Casing**: Check ticket description for "API casing: ..." specification. If specified, verify all API fields use that casing (snake_case, camelCase, PascalCase, kebab-case). If not specified, default expectation is snake_case for external APIs.
    - **Field Names**: Request/response fields match API contracts (check against ticket or API docs)
    - **HTTP Methods**: GET/POST/PUT/DELETE/PATCH usage is semantically correct
    - **Status Codes**: 200/201/400/404/500 codes match scenarios (success, validation, not found, server error)
    - **Error Responses**: Error response format matches API error contract
    - If ticket mentions OR code touches NODES API: load `/nodes-api` skill (if available), verify implementation matches spec
    - If ticket mentions OR code touches OpenADR: load `/openadr-api` skill (if available), verify VEN/VTN patterns match spec
    - If ticket mentions OR code touches Axle Energy or Easee devices/state: load `/axle-energy-api` skill (if available), verify integration matches spec

    ### 3. Test Quality (Not Just Coverage)
    - **Test Existence**: Every new public method MUST have tests. If test files missing for new prod code, set approved=false
    - **Happy Path Coverage**: Primary success scenarios are tested
    - **Error Handling Coverage**: Failure cases and exceptions are tested (not just happy path)
    - **Edge Cases**: Boundary conditions tested (null inputs, empty arrays, max values, etc)
    - **Mock Appropriateness**:
      - Unit tests mock external dependencies appropriately
      - Integration tests use real implementations where needed
      - Not overly mocked - tests actually verify behavior, not just mock interactions
    - **Test Realism**: Tests verify actual behavior, not just coverage padding
    - **Test Names**: Test names clearly describe what's being tested
    - If backend changed: Run architecture tests with ./gradlew test --tests "*ArchitectureTest*"

    ### 4. Code Quality
    - No obvious security vulnerabilities (input validation, SQL injection prevention, XSS)
    - No debug code, orphan TODOs, or commented-out code
    - Error handling is appropriate (no swallowed exceptions)
    - Code follows project conventions (check CLAUDE.md if unsure)

    ### 5. Assumption Validation (if assumptions provided)
    - **Match check**: Does implementation actually follow documented assumptions?
    - **Reasonableness**: Were the decisions sensible given the task context? Challenge questionable assumptions.
    - **Alternatives considered**: Should different approaches have been chosen?
    - **Risk assessment**: Could any assumptions lead to future issues?
    - **Scope creep**: Any undocumented decisions or work outside planned scope?

    ## Instructions
    - Use git diff to see changes
    - Read the changed files
    - Compare implementation against the task intent above
    - If code interacts with external APIs, use /nodes-api, /openadr-api, or /axle-energy-api skills (if available) to load specs
    - Check test coverage for new code - ensure tests are meaningful, not just coverage padding
    - If assumptions were provided, validate each one

    ## Output Format
    Output your response as JSON with:
    - approved: true if review passes, false if issues found
    - findings: array of what was checked/found, each with:
      - category: "intent" | "boundaries" | "quality" | "assumptions" | "test_quality" | "api_verification"
      - status: "pass" | "issue" | "fixed"
      - detail: brief description
    - summary: one-line summary (e.g., "intent aligned, 2 quality issues fixed")

    Example:
    {{"approved": true, "findings": [{{"category": "intent", "status": "pass", "detail": "implements task scope correctly"}}, {{"category": "test_quality", "status": "pass", "detail": "happy path and error cases tested"}}, {{"category": "api_verification", "status": "pass", "detail": "all fields use snake_case per ticket spec"}}, {{"category": "quality", "status": "fixed", "detail": "removed debug println"}}], "summary": "approved with 1 fix applied"}}

    IMPORTANT: If code doesn't match task intent, set approved=false.

fix:
  description: Fix issues from code review
  template: |
    Fix these code review issues:

    {issues}

    ## Instructions
    - Make the fixes
    - Verify they work by reading the code
    - Don't run tests yet (that comes next)

review_light:
  description: Fast sanity check for loop iterations (used by verify_step)
  template: |
    Quick review of changes for: {task_title}

    Check ONLY these basics (be fast, not thorough):
    1. Code compiles/parses correctly (no syntax errors visible)
    2. No obvious bugs or typos
    3. Changes are related to the task (not random edits)
    4. No debug code left behind (println, console.log, etc.)

    Use git diff to see changes. Be quick - this is a sanity check, not a deep review.

    Output JSON:
    - If approved: {{"approved": true, "issue": null}}
    - If issue found: {{"approved": false, "issue": "brief description"}}

completion_check:
  description: Verify a step's described goal was actually achieved
  template: |
    Verify that this step's goal was achieved in the code changes.

    ## Step {step_id}: {step_desc}

    ## Planned files
    {files_context}

    ## Instructions
    1. Run `git diff HEAD` to see what changed
    2. Compare the diff against the step description above
    3. If step says "Create X" — verify X exists with real content
    4. If step says "Add method Y" — verify method Y exists
    5. Check each planned file was handled (created/modified/deleted)

    Focus ONLY on completeness. Don't check quality, style, or tests.
    A step is complete if all described work items are present, even if imperfect.

    Output JSON:
    - If complete: {{"complete": true, "missing": null}}
    - If incomplete: {{"complete": false, "missing": "brief description of what's missing"}}

coverage_fix:
  template: |
    Coverage verification failed (below 80%).

    ## Changed Files
    {changed_files}

    ## Error Output
    {error_output}

    ## Instructions
    1. Run: ./gradlew koverHtmlReport
    2. Check build/reports/kover/html/ for uncovered lines
    3. Add meaningful tests for uncovered code
    4. Run: ./gradlew koverVerify to check

    Focus on: public methods, branches, error paths.
    Do NOT pad with meaningless tests.

final_review:
  description: Final comprehensive review prompt for skill or standard review
  template: |
    All implementation steps complete. Perform final comprehensive review.

    ## Ticket Context
    **Main Ticket:** {ticket_id} - {title}

    {description}

    {subtasks_context}

    ## Commits Made
    {commit_messages}

    ## Plan Steps Completed
    {plan_steps}

    ## Assumptions Made During Implementation
    {assumptions}

    **IMPORTANT**: Review whether these assumptions were reasonable and well-justified. Challenge any that seem questionable or could lead to issues.

    ## Review ALL Changes For:

    ### 1. Ticket Intent Alignment
    - Does the complete implementation solve what the ticket asked for?
    - Any missing functionality from the ticket requirements?
    - Any scope creep beyond the ticket?

    ### 2. API Boundaries & Specifications
    - API casing matches ticket specification (check ticket for "API casing: ..." or default to snake_case)
    - Field names match API contracts
    - HTTP methods semantically correct (GET for reads, POST for creates, etc)
    - Status codes appropriate (200/201 success, 400 validation, 404 not found, 500 errors)
    - Error response format consistent

    ### 3. Test Quality - Be Specific
    - **Coverage of scenarios**:
      - Happy path: Primary success scenarios tested
      - Error handling: Failure and exception paths tested
      - Edge cases: Null inputs, empty collections, boundary values
    - **Test realism**:
      - Not just coverage padding - tests verify actual behavior
      - Unit tests mock appropriately, integration tests use real implementations
      - Tests have descriptive names
    - **Completeness**: Every new public method has tests

    ### 4. Code Quality
    - No debug code (console.log, println, commented code)
    - No orphaned TODOs
    - Security: No obvious vulnerabilities (input validation, injection prevention)
    - Error handling: No swallowed exceptions
    - Follows project conventions

    Return JSON:
    - approved: true if ready, false if issues
    - action_items: list of specific changes needed (empty if approved), each with:
      - priority: "critical" (blocks merge - missing tests, broken API contract, security) | "important" (should fix - code quality, minor API issues) | "minor" (nice to have - style, refactoring suggestions)
      - action: specific change needed with file/location if applicable
    - summary: one-line outcome

    Example:
    {{"approved": false, "action_items": [{{"priority": "critical", "action": "Add test for null input in DeviceService.register()"}}, {{"priority": "important", "action": "Fix API casing in DeviceResponse: externalId → external_id"}}, {{"priority": "minor", "action": "Extract retry count magic number to constant"}}], "summary": "1 critical, 2 other issues"}}
